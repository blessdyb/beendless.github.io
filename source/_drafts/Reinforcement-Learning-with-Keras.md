---
title: 强化学习
date: 2018-12-18 22:42:33
categories: CS
tags:
    - Machine Learning
    - Reinforcement Learning
---

强化学习在目前多个领域取得了不错的成绩。其目标是实现一些能够学习并适应环境变化的算法。可以通过所在环境对于算法所做选择生成的反馈信息的处理来实现---最终得到最优化的解决方案。这种机制是基于机器学习的基础的概念来试图模拟人类的大脑活动。例如，人类的大脑会主动地趋使我们去追求令我们感到愉悦，满足的事物。当我们人类感觉到满足时，大脑中就会产生某种信号来激励并强化这种形为。

这其中一个比较重要的角色是记忆，或者说是存储(memory)。记忆中经验的积累，可以帮助我们在未来针对当前环境，对比之前已经获得的经验来做出更好的选择。著名的心理学上的动物实验对强化学习的建立有很大的理论影响。实验中，心理学家们针对动物的行为来进行惩罚或奖励，这样通过多次实验会，动物的行为就会趋向于得到更多的奖励。基于这个理论，对于一个给定的目标，强化学习算法的实体(Agent)将会通过与外部的环境(environment)持续交互的过程中, 产生一系列行为(action)，算法所在环境在作用过程的每一个状态(state)下, 针对这一系列行为会产生对应的奖励(reward)与惩罚(penalty)，基于得到的奖励与惩罚的计算而得到一个最优化并最接近给定目标的结果。

以下是通常一个强化学习算法的实现框架
1. 实现一个可与`environment`进行交互的`agent`
2. 对`environment`的`state`进行监测
3. 针对当前的`state`来选择一个最优化的策略
4. 执行`action`并计算之后`environment`给出的`reward` 或 `penalty`
5. 重复 3 ~ 4直到`agent`所获得的`reward`积累到一个期望的结果

`Agent`具有很强的结果导向的行为。但在一个未知的环境中`agent`的行为通常是无法预先确定的或者只能部分确定。`Agent`必须通过与`environment`的持续交互来学习，这个是一个持续试错的过程。在这个过程中，`agent`根据某种`policy`执行一个`action`, 与之对应的，`environment`更新其`state`, 并返回一个`reward`/`penalty`, 这个`reward`/`penalty`的值可以给`agent`反馈相关的信息--- 之前所选择的action是否是一个最优化的值。

如果我们想自动化学习强化学习的过程，我们必须能够对`environment`的特性所有了解，通常所有的`environment`都可以简化并使用`MDP`(马尔可夫过程)来描述。

`MDP`是一个特殊的随机过程，简而言之，它将来的状态分布只取决于现在，跟过去无关。可以使用如下公式来表示

s<sub>t</sub> + 1 = δ (s<sub>t</sub>, a<sub>t</sub>)

其中 `δ `来表示状态方程。

在一个给定的`MDP`中，在任何一个时刻，过程应该处于一个`state` `s` (`s ∈ S`)。 在此状态下，`agent`可以选择`action` `a` (`a ∈ A`)。在执行了`action` `a` 后，此过程会转移到一个新的`state` `s'`, 并对应之前的`action` 给出一个`reward` `r(s, s')`。

简而言之，我们只需要关注以下几个函数方程的实现:
* `Reward` 方程 —— 定义了强化学习的目标。给定某一时刻`t`, `environment`所处的`state` 及在此`state`下`agent`所选择并执行的`action`, 由此方程可求得当前的`reward`。
* `Policy` 方程 —— 定义了`agent`在给定某一时刻`t`及 `environment`所处的`state`，`agent`如何选取`action`。
* `Value` 方程 —— 定义了对于`agent`来说，如何评估当前的`state`。它与从当前`state`开始，`agent`持续与`environment`交互所能得到的累积`reward`的最大值相同。(注: 所有的`action`在执行后，都会有一个短期的`reward`和长期的`reward`)
* `action-value` 方程 —— 指定`agent`的`action`及所处的`environment`的`state`, 在`action`执行后所能得到的`reward`的最大值。（考虑长期影响的累积值)

核心要解决的是一个长期累积`reward`的计算，理论上需要遍历所有的`state`下所有可能选取的`action`，同时还要递归地顺次求取`reward`, 这是一个不现实的方案（探索——利用困境）。我们的目标是每一次获得`尽可能高`的`reward`, 从而获得最佳的结果。